\documentclass[aps,prc,reprint,amsmath,nofootinbib,superscriptaddress]{revtex4-1}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage[inline]{enumitem}

\usepackage{graphicx}
\graphicspath{{fig/}}

\newcommand{\trento}{T\raisebox{-0.5ex}{R}ENTo}
\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\nch}{N_\text{ch}}
\newcommand{\npart}{N_\text{part}}
\newcommand{\sqrts}{\sqrt{s_{NN}}}
\newcommand{\T}{\tilde{T}}
\newcommand{\Qs}[1]{Q_{s,\text{#1}}}
\newcommand{\vnk}[2]{v_#1\{#2\}}
\newcommand{\paddedhline}{\noalign{\smallskip}\hline\noalign{\smallskip}}
\newcommand{\order}[1]{$\mathcal O(10^{#1})$}

% Convenient figure macro.  Usage:
%
%   \fig[placement specifier = t]{filename}{caption}
%
% This creates a figure environment, includes the given filename as graphics,
% puts the given caption below the graphics, and labels it 'fig:filename'.
% The optional placement specifier defaults to 't' and is passed directly to
% the figure environment.
% Use \fig* to make a figure* environment, i.e. a wide figure.
\usepackage{xparse}
\NewDocumentCommand\fig{sO{t}mm}{
  \begin{figure\IfBooleanT{#1}{*}}[#2]
    \includegraphics{#3}
    \caption{\label{fig:#3}#4}
  \end{figure\IfBooleanT{#1}{*}}
}


\begin{document}


\title{Determining quark-gluon plasma initial conditions and medium \\ properties from model-to-data simulations with minimal bias}

\author{Jonah E.\ Bernhard}
\author{J.\ Scott Moreland}
\affiliation{Department of Physics, Duke University, Durham, NC 27708-0305}

\author{Jia Liu}
\affiliation{Department of Physics, The Ohio State University, Columbus, OH 43210-1117}

\author{Steffen A.\ Bass}
\affiliation{Department of Physics, Duke University, Durham, NC 27708-0305}

\date{\today}

\begin{abstract}
We aim to determine the quark-gluon-plasma initial conditions created in ultra-relativistic heavy-ion collisions and its temperature dependent transport coefficients utilizing Bayesian statistics and a multi-parameter model-to-data comparison.
The study is performed using \trento, a recently developed parametric initial condition model for relativistic nuclear collisions which is able to interpolate among a general class of saturation based particle production scenarios.
The parameterization is compared to explicit calculations in Color-Glass Condensate (CGC) effective field theory and embedded in a realistic hybrid model which couples event-by-event viscous hydrodynamics to a hadronic cascade.
We find that the parameterized initial conditions are highly constrained by bulk observables and provide a first set of constraints on the temperature dependence of the specific shear- and bulk-viscosities of the quark-gluon plasma.
\end{abstract}

\maketitle


\section{Introduction}

Simulations based on relativistic viscous hydrodynamics have been highly successful describing a wealth of bulk observables in heavy-ion collisions at the Relativistic Heavy-Ion Collider (RHIC) in Brookhaven, NY and the Large Hadron Collider (LHC) in Geneva, Switzerland.
Initially, the success of hydrodynamic simulations was primarily qualitative. The framework elegantly described a number experimental phenomena, for example the existence of large azimuthal particle correlations, the mass ordering of these correlations and their characteristic momentum dependence.

Modern hydrodynamic simulations have greatly improved upon the successes of first-generation models.
The addition of dissipative corrections to ideal hydrodynamics \cite{Muronga:2004sf, Chaudhuri:2006jd, Romatschke:2007mq, Dusling:2007gi, Song:2007ux, Luzum:2008cw}, event-by-event fluctuations in the colliding nuclei \cite{Alver:2008zza, Alver:2010gr} and modern lattice quantum chromodynamics (QCD) calculations for the quark-gluon plasma (QGP) equation of state \cite{Bazavov:2009zn, Borsanyi:2013bia, Bazavov:2014pvz} are just a few examples of developments which have dramatically improved the agreement of hydrodynamic models with experiment.

These developments have positioned hydrodynamic modeling to evolve beyond a qualitative science and extract intrinsic properties of hot and dense QCD matter with quantitative uncertainties. A primary goal of the ongoing effort is to determine the temperature dependence of the QGP transport coefficients such as the specific shear viscosity $\eta/s$ which is theorized to reach a lower bound ${(\eta/s)(T) \ge 1/4\pi}$ near the QGP phase transition temperature \cite{Policastro:2001yc, Kovtun:2004de}.

A prominent source of uncertainty in determining QGP transport coefficients from hydrodynamic model-to-data comparison is in modeling the initial stages of the collision.
Model predictions vary with the choice of initial conditions and hence prefer hydrodynamic transport coefficients, e.g.\ QGP shear and bulk viscosities, which differ from calculation to calculation.
For example, an estimate of the effective (constant) QGP shear viscosity $(\eta/s)_\text{QGP}$ needed to fit spectra and flows at RHIC found ${1 \le 4 \pi(\eta/s)_\text{QGP} \le 2.5}$, where the uncertainty was attributable to discrepancies in $(\eta/s)_\text{QGP}$ extracted with two different hydrodynamic initial condition models \cite{Song:2010mg}.

The ongoing effort to constrain current bounds on the QGP shear viscosity typically focuses on improving theoretical descriptions of the initial conditions \cite{Schenke:2012wb, Niemi:2015qia} as well as the addition of new sensitive observables to assess the validity of each model's inherent assumptions and approximations.
The process thus defines an iterative cycle in which theory calculations are embedded in hydrodynamic transport simulations and analyzed against a comprehensive list of bulk observables to determine the likelihood distributions of the model parameters.

Model optimization and comparison is complicated by the high-dimensional nature of the underconstrained parameter space.
In addition to free parameters which describe the QGP transport coefficients, hydrodynamic simulations depend on a number of auxiliary parameters such as an effective nucleon width, QGP thermalization time, and chemical freeze-out temperature which require simultaneous optimization.
Evaluating hydrodynamic models at a single point in parameter space may require tens of thousand of hydrodynamic simulations and thus brute force optimization techniques quickly become intractable.

One solution to the model optimization problem is the use of modern Bayesian methods designed to tune computationally intensive models which depend on free parameters that live in large and highly correlated parameter spaces \cite{OHagan:2006ba, Higdon:2008cmc, Higdon:2014tva,Wesolowski:2015fqa}.
A given model is first evaluated using a relatively small number of parameter configurations corresponding to discrete points in parameter space, and the model evaluations are used to train a Gaussian process emulator which interpolates model predictions between the training points \cite{Rasmussen:2006gp}.
A standard Markov chain Monte Carlo algorithm is then be used to explore the parameter space interpolated by the emulator and isolate regions of parameter space which optimally replicate experiment.

Bayesian methods have been applied to heavy-ion collisions in several previous studies \cite{Novak:2013bqa, Pratt:2015zsa, Bernhard:2015hxa, Sangaline:2015isa}, including simulations initialized with a two-component Monte Carlo Glauber (MC-Glb.) model and Color-Glass Condensate (CGC) initial conditions described by the Kharzeev-Levin-Nardi (MC-KLN) model \cite{Bernhard:2015hxa}.
Future studies might expand this coverage to more modern calculations of the QGP initial conditions in order to systematically constrain hydrodynamic transport coefficients and auxiliary parameters for each initialization model described in the literature.
Once the models are appropriately optimized, the accuracy of the different theory calculations can be assessed using an appropriately chosen goodness of fit measure or, as typically used in Bayesian methodology, an integrated likelihood that a given model describes experiment.

An alternative approach to this method of direct model validation is to parameterize the QGP initial conditions and apply Bayesian parameter estimation to determine systematic constraints on the initial energy (or entropy) deposited by competing models, as employed in \cite{Novak:2013bqa, Pratt:2015zsa, Sangaline:2015isa}.
The authors parameterized the initial conditions using a saturation inspired mapping similar to an optical wounded nucleon model but with additional parameters to incorporate modifications to particle production expected in Color-Glass Condensate effective field theory.
Bayesian analysis was then used to constrain parameterized forms of the event-averaged initial conditions and temperature dependence of the QGP equation of state using viscous hydrodynamics coupled to a hadronic afterburner.

In this work we extend previous efforts to parameterize and constrain the QGP initial conditions using \trento , a recently developed parametric initial condition model designed to interpolate a subspace of all initialization models, including, but not limited to, parameterizations that mimic specific calculations in Color-Glass Condensate effective field theory \cite{Moreland:2014oya}.
The parametric model generalizes trivially to incorporate fluctuations of the nucleon coordinates and minimum bias proton-proton yields and is used to conduct the first Bayesian analysis of QGP initial conditions which parameterizes both the magnitude of event-by-event fluctuations and the strength of saturation effects which modulate average particle production.
We show that the experimental data imparts simultaneous constraints on the QGP initial conditions and medium properties, and compare these constraints to specific calculations in the literature.


\section{Modeling the QGP evolution}

The heavy-ion collision transport dynamics are modeled in a multi-stage approach which uses relativistic viscous hydrodynamics for the time-evolution of the QGP medium and a microscopic Boltzmann equation to simulate the dynamics of the system after hadronization.

\subsection{Hydrodynamics and Boltzmann transport}

The hydrodynamic evolution is solved using an improved version of VISH2+1 \cite{Song:2007ux}, a stable, extensively tested implementation of boost-invariant viscous hydrodynamics which has been updated to handle fluctuating event-by-event initial conditions \cite{Shen:2014vra} and incorporate bulk viscous corrections with shear and bulk coupling \cite{Denicol:2014vaa}.
The equations of motion are described by the second-order Israel-Stewart equations in the 14-moment approximation \cite{Stewart59, Israel43, ISRAEL1976213} which yields a set of relaxation equations
\begin{subequations}
  \begin{align}
    \tau_\Pi \Pi + \dot{\Pi} &=
      -\zeta \theta - \delta_{\Pi\Pi} \Pi\theta + \phi_1 \Pi^2
      + \lambda_{\Pi\pi} \pi^{\mu\nu} \sigma_{\mu\nu} \nonumber \\
      &\qquad + \phi_3 \pi^{\mu\nu}\pi_{\mu\nu}, \\
    \tau_\pi \dot{\pi}^{\langle \mu\nu \rangle} + \pi^{\mu\nu} &=
      2\eta\sigma^{\mu\nu} + 2\pi_\alpha^{\langle \mu} w^{\nu \rangle \alpha}
      - \delta_{\pi\pi} \pi^{\mu\nu} \theta \phi_7 \pi_\alpha^{\langle \mu}
        \pi^{\nu \rangle \alpha} \nonumber \\
      &\qquad - \tau_{\pi\pi} \pi_\alpha^{\langle \mu}\sigma^{\nu \rangle \alpha}
      + \lambda_{\pi\Pi} \Pi \sigma^{\mu\nu} + \phi_6 \Pi \pi^{\mu\nu},
  \end{align}
  \label{eq:relaxation}
\end{subequations}
for the bulk viscous pressure $\Pi$ and shear-stress tensor $\pi^{\mu\nu}$.
The terms $\zeta$ and $\eta$ are the first-order bulk and shear transport coefficients, and the parameters $\tau_\Pi$ and $\tau_\pi$ their second-order bulk and shear relaxation times.
The remaining transport coefficients multiplying shear and bulk terms on the right can then be expressed as functions of the relaxation times and other thermodynamic quantities and have been determined in the limit of small but finite masses in reference \cite{Denicol:2014vaa}.

The relaxation equations in Eq.~\eqref{eq:relaxation} are then used to solve the hydrodynamic evolution equations,
\begin{equation}
  \partial_\mu T^{\mu\nu} = 0, \quad T^{\mu\nu} = e u^\mu u^\nu  - \Delta^{\mu\nu} (P_0 + \Pi) + \pi^{\mu\nu},
  \label{eq:conservation}
\end{equation}
for the energy-momentum tensor $T^{\mu\nu}$, provided a set of initial conditions for the energy density $e(x,y)$ and four-velocity $u^\mu(x,y)$ as well as an equation of state to interrelate the energy density $e$, ideal pressure $P_0$, and temperature $T$ of each fluid cell in local thermal equilibrium.

We use a modern QCD equation of state (EoS) based on continuum extrapolated lattice calculations at zero baryon density published by the HotQCD collaboration \cite{Bazavov:2014pvz} and blended into a hadron resonance gas EoS in the interval $110 \le T \le 130$~MeV using a smoothstep interpolation function \cite{Moreland:2015dvc}.
The HotQCD EoS, characterized by the parameterized interaction measure $\theta^{\mu\mu}(T) = (e - 3p)/T^4$, has been compared to additional state-of-the-art calculations by the Wuppertal-Budapest collaboration and shown to agree within published errors \cite{Bazavov:2014pvz}.
The two parameterizations were also studied in a recent error analysis at RHIC energies which quantified the effect of systematic lattice EoS discrepancies and statistical continuum extrapolation errors on hydrodynamic observables \cite{Moreland:2015dvc}.
The effect of these errors on mean $p_T$, elliptic flow $v_2$, and triangular flow $v_3$ was found to be order 1\% and hence is expected to be negligible in the present analysis.

We seek to constrain, among other quantities, the temperature dependence of the shear and bulk transport coefficients $\eta(T)$ and $\zeta(T)$ in Eq.~\eqref{eq:relaxation}.
The QGP specific shear viscosity $\eta/s$ is qualitatively expected to reach a global minimum near the QGP phase transition temperature and hence is parameterized using a simple form,
\begin{equation}
  (\eta/s)(T) =
  \begin{cases}
    (\eta/s)_\text{min} + (\eta/s)_\text{slope} (T - T_c) & T > T_c \\
    (\eta/s)_\text{hrg}                                   & T \le T_c
  \end{cases}
  \label{eq:etas}
\end{equation}
where we fix $T_c = 0.155$~GeV using the approximate pseudo-critical phase transition temperature of the HotQCD equation of state, and $(\eta/s)_\text{min}$, $(\eta/s)_\text{slope}$, and $(\eta/s)_\text{hrg}$ are constants which we wish to determine.
For the temperature dependent bulk viscosity, we use the parameterization constructed in Ref.~\cite{Denicol:2009am, Ryu:2015vwa},
\begin{equation}
  (\zeta/s)(T) =
  \begin{cases}
    \begin{aligned}
      C_1 &+ \lambda_1 \exp [(x-1)/\sigma_1]  \\ &+ \lambda_2 \exp [ (x-1)/\sigma_2]
    \end{aligned}
    &T < T_a \\[3ex]
    A_0 + A_1 x + A_2 x^2 &T_a \le T \le T_b \\[2ex]
    \begin{aligned}
      C_2 &+ \lambda_3 \exp [-(x-1)/\sigma_3]  \\ &+ \lambda_4 \exp [-(x-1)/\sigma_4]
    \end{aligned}
    &T > T_b
  \end{cases}
  \label{eq:zetas}
\end{equation}
with dimensionless parameter $x = T/T_0$ and parameterization coefficients
\begin{align*}
  &C_1=0.03, ~~C_2=0.001, \\
  &A_0=-13.45, ~~A_1=27.55, ~~A_2=-13.77, \\
  &\sigma_1=0.0025, ~~\sigma_2=0.022, ~~\sigma_3=0.025, ~~\sigma_4=0.13, \\
  &\lambda_1=0.9, ~~\lambda_2=0.22, ~~\lambda_3=0.9, ~~\lambda_4=0.25, \\
  &T_0 = 0.18 \text{ GeV}, ~~T_a = 0.995\, T_0, ~~T_b = 1.05\, T_0.
\end{align*}
The $(\zeta/s)(T)$ curve in Eq.~\eqref{eq:zetas} is then rescaled by an overall normalization factor $(\zeta/s)_\text{norm}$ which is a free parameter to be constrained by data.
The normalization factor used in this work is thus equal to unity when the rescaled $(\zeta/s)(T)$ parameterization in Eq.~\eqref{eq:zetas} equals the approximate best fit parameterization determined in Ref.~\cite{Ryu:2015vwa}.

The hybrid model used in the present analysis switches from hydrodynamic field equations to a microscopic kinetic description of the medium once the fluid expands, cools, and hadronizes. Kinetic descriptions are well suited to describe late stages of the collision where the inter-particle mean free path approaches the system size and approximate local thermal equilibrium breaks down.
Kinetic models also naturally describe species dependent kinetic freeze-out and hadronic feed down dynamics which are difficult to incorporate in purely hydrodynamic models.
More detailed discussion of the advantages of hybrid models can be found in references \ref{?}.

The transition from viscous hydrodynamics to a hadronic cascade is performed along a constant isotherm $T_\text{sw}$ using the iSS particle sampler \cite{Shen:2014vra, Qiu:2013wca} according to the Cooper-Frye prescription \cite{Cooper:1974mv},
\begin{equation}
  \frac{dN}{dy p_T dp_T d\phi_p} =
    \int_\Sigma \frac{g}{(2\pi)^3} (f_0(x,p) + \delta f(x,p)) p^\mu d^3 \sigma_\mu,
  \label{eq:cooper_frye}
\end{equation}
where $E$ and $p$ are the energy and momentum of the sampled particle, $f_0$ the ideal distribution function, $\delta f$ its viscous corrections, and $d^3\sigma_\mu$ is an element of the isothermal freeze-out surface $\Sigma$.

In general, the viscous corrections to the distribution function $\delta f(x, p)$ in Eq.~\eqref{eq:cooper_frye} include both shear and bulk contributions.
The shear correction to the distribution function is given by,
\begin{equation}
  \delta f_\text{shear}(x^\mu, p^\mu) =
    f_0(x^\mu, p^\mu)(1 \pm f_0(x^\mu, p^\mu))
    \frac{\pi_{\mu\nu}p^\mu p^\nu}{2(e+P)T^2}
\end{equation}
where $f_0$ is the ideal distribution function, $\pi_{\mu\nu}$ the shear stress tensor, $p^\mu$ the particle momentum and $e$, $P$ and $T$ the fluid's energy density, pressure and temperature.
The bulk viscous correction, on the other hand, is described by multiple formulations in the literature which predict significantly different behavior when either the bulk viscosity $\zeta$ or the transverse momentum $p_T$ are large \cite{Dusling:2011fd, Noronha-Hostler:2013gga}. The present study neglects bulk viscous corrections to the distribution function $\delta f_\text{bulk}$ at particlization, based on the expectation that the corrections are small for the specific $(\zeta/s)(T)$ curve Eq.~\eqref{eq:zetas} used in this analysis. The approximation, admittedly, is not great and leads to, e.g.\ order 10\% errors in the magnitude of mean $p_T$ at particlization. We thus refrain from making any quantitative statements regarding the magnitude of bulk viscosity preferred by the analysis and emphasize that a more realistic $(\zeta/s)(T)$ curve and careful treatment of $\delta f_\text{bulk}$ are necessary to constrain the QGP bulk viscosity with precision.

Once the fluid is converted into hadrons, the subsequent microscopic dynamics are modeled using the UrQMD hadronic afterburner \cite{Bass:1998ca, Bleicher:1999xi}.
The UrQMD model propagates all produced hadrons along classical trajectories, samples their stochastic binary collisions and simulates their resonance decays until the system becomes too dilute to continue interacting.
The calculation is performed using a Monte Carlo realization of the Boltzmann equation
\begin{equation}
  \frac{df_i(x,p)}{dt} = \mathcal{C}_i(x, p)
\end{equation}
where $f_i(x,p)$ represents the phase space density of species $i$ and $\mathcal{C}_i$ is its corresponding collision kernel.
Once the particles cease interacting, the position, momentum, and particle type is saved to file.

\subsection{Parametric initial conditions}

The hydrodynamic equations of motion necessitate initial conditions for the energy density $e$, four-velocity $u^\mu$, bulk pressure $\Pi$, and shear stress tensor $\pi^{\mu\nu}$ at the thermalization time $\tau_0$ when the fluid attains local thermal equilibrium. The initial conditions are typically procured using one of two types of models:
\begin{enumerate*}[label={\arabic*)}]
  \item dynamical models derived from approximations of QCD or related quantum field theories which describe the full time evolution of the system as it approaches local thermal equilibrium \cite{Schenke:2012wb, vanderSchee:2013pia, Wang:1991hta}, and
  \item simpler non-dynamical models which map the state of the system at time $\tau=0^+$ directly to time $\tau=\tau_0$ when the system thermalizes \cite{}.
\end{enumerate*}

The importance of pre-equilibrium dynamics was quantified in a recent study by initializing hydrodynamic simulations with a free streaming phase (zero coupling) and switching to hydrodynamics (strong coupling) after different periods of time \cite{Liu:2015nwa}. The authors showed that although free streaming never leads to thermalization, it can be used to bracket the influence of pre-equilibrium dynamics on the medium evolution as the pre-equilibrium coupling strength is expected to fall between the free streaming and hydrodynamic limits. The analysis found a preference for a brief free streaming phase ${\tau_\text{fs} \lesssim 1}$~fm/c, but the effect on hydrodynamic bulk observables was small and modifications to the preferred value of the QGP specific shear viscosity $\eta/s$ were less than 10\%. In reality, the magnitude of the effect is expected to be even smaller as the pre-equilibrium coupling strength is necessarily non-zero.

We choose to neglect pre-equilibrium dynamics in the present study, and set the fluid four-velocity to zero ${u^\mu = (1,0,0,0)}$ as well as the shear and bulk viscous tensors $\pi^{\mu\nu}$ and $\Pi$ which quickly relax to their Navier-Stokes values. This reduces the initial conditions to a thermal energy density $e(\tau_0, x, y, \eta)$ which can be expressed as an entropy density via the QCD EoS. The initial conditions in boost-invariant hydrodynamics can thus described by an entropy density profile
\begin{equation}
  s(\tau_0, x, y)\vert_{\eta=0} = f(\T_A, \T_B)
  \label{eq:mapping}
\end{equation}
where the mapping $f$ is some static function of the density of nuclear matter in each nucleus which participates in inelastic collisions denoted $\T$, constructed by summing over the participant proton thicknesses
\begin{equation}
  \T(x, y) = \sum\limits_{i=1}^{N_\text{part}} \gamma_i\, T_p(x - x_i, y - y_i),
  \label{eq:participant}
\end{equation}
where the relative nucleon coordinates of each nucleus are shifted by a random impact parameter such that ${\T_{A,B}(x, y) = \T(x \pm b/2, y)}$. For the proton thickness function we use a Gaussian
\begin{equation}
  T_p(\vec{x}) = \frac{1}{2\pi w^2} \exp\bigg(\!-\frac{x^2 + y^2}{2 w^2}\bigg).
\end{equation}
with nucleon width $w$ taken as a free parameter. The random weight factor $\gamma_i$ multiplying each nucleon in Eq.~\eqref{eq:participant} is added to account for minimum bias proton-proton multiplicity fluctuations and is sampled from a Gamma distribution with unit mean and variance $1/k$, where $k$ is a tunable shape parameter.

The inelastic nucleon-nucleon participation probability, and hence the density of participant matter, can be calculated from a standard Glauber formalism,
\begin{equation}
  P_\text{coll}(b) =
    1 - \exp\biggl[
      -\sigma_{gg} \int d^2x_\perp T_p(|\vec{x}|) T_p(|\vec{x} - \vec{b}|)
    \biggr]
\end{equation}
where $b$ is the impact parameter between two nucleons and the partonic cross section $\sigma_{gg}$ is tuned to fit the inelastic nucleon-nucleon cross section
\begin{equation}
  \int 2 \pi b\, db\, P_\text{coll}(b) = \sigma_\text{NN}^\text{inel}
\end{equation}
at the desired beam energy. The collision cross sections thus enter through the calculation of $\T_A$ and $\T_B$ which are determined \emph{independently} of the function $f$ used to convert local participant density into entropy deposition.

\fig{thickness}{
  Cross section of the participant nucleon density in a mid-central Pb+Pb collision at $\sqrts=2.76$ TeV as a function of the transverse coordinate $x$ parallel to impact parameter $\vec{b}$.
  The gray band indicates the region bounded by the minimum and maximum values of the local participant thickness functions $T_A$ and $T_B$, while the blue band indicates the region spanned by the generalized mean of $T_A$ and $T_B$ with parameter $-1<p<1$.
  The solid blue line shows an example of a discrete mapping specified by a generalized mean with $p=0$.
}

At this point it becomes necessary to specify the function $f$ in Eq~\eqref{eq:mapping} which maps participant matter to local entropy deposition. In general, the function $f(\T_A, \T_B)$ is assumed to be symmetric, monotonically increasing and smooth. In this work we employ a parameterization for $f$ motivated in Ref.~\cite{Moreland:2014oya} based on a functional form known as the generalized mean,
\begin{align}
  s \propto \left( \frac{\T_A^p + \T_B^p}{2} \right)^{1/p}.
  \label{eq:genmean}
\end{align}
The parameterization introduces two free parameters, a normalization prefactor and a continuous entropy deposition parameter $p\in(-\infty, \infty)$ which interpolates between different types of entropy deposition schemes. For ${p=(1, 0, -1)}$, the generalized mean reduces to the well known arithmetic, geometric and harmonic means, and for $p\rightarrow \pm\infty$ it asymptotes to minimum and maximum functions,
\begin{equation}
  \newlength{\extraspace}
  \setlength{\extraspace}{0.5ex}
  s \propto
  \begin{cases}
    \max(\T_A, \T_B) & p \rightarrow +\infty, \\[\extraspace]
    (\T_A + \T_B)/2 & p = +1, \hfill \text{ (arithmetic)} \\[\extraspace]
    \sqrt{\T_A \T_B} & p = 0, \hfill \text{ (geometric)} \\[\extraspace]
    2\, \T_A \T_B/(\T_A + \T_B) & p = -1, \hfill \text{ (harmonic)} \\[\extraspace]
    \min(\T_A, \T_B) & p \rightarrow -\infty.
  \end{cases}
  \label{eq:means}
\end{equation}

In Ref.~\ref{?} we motivate the generalized mean ansatz using basic physical constraints and phenomenological observations, but perhaps the simplest explanation of the ansatz is to examine the effect of the mapping on a realistic event. Fig.~\ref{fig:thickness} shows a cut-away of a lead-lead collision at $\sqrts=2.76$~TeV, sliced along the direction of impact parameter $\hat{b}$. At each point in the transverse plane there are two relevant scales of interest, the minimum participant density $\T_\text{min}$ and the maximum participant density $\T_\text{max}$. The gray band marks the region spanned by $\T_\text{min}$ and $\T_\text{max}$, while the blue band/line show the generalized mean acting on the participant density of each nucleus with different values of the parameter $p$. The figure shows that decreasing $p$ pulls the generalized mean towards the minimum of $\T_A$ and $\T_B$ while increasing $p$ pushes it to the maximum of the two. The generalized mean ansatz thus parameterizes asymmetric entropy deposition, or in the parlance of Color-Glass Condensate theory, the intensity of saturation effects on local gluon production.

At this point the reader may complain that while the generalized mean parameterizes entropy deposition in asymmetric regions of the collision $\T_\text{min} < \T_\text{max}$, it asserts a particular scaling law for entropy deposition in symmetric regions of the collision where $\T_\text{min} = \T_\text{max}$. In particular,
\begin{equation}
  f(p; \alpha \T, \alpha \T) = \alpha \T.
  \label{eq:homogenous}
\end{equation}
This property of the generalized mean is known as scale invariance or homogeneity. While it is difficult to empirically prove or disprove exact scale invariance, multiple experimental observations indicate that it holds to very good approximation. For example, it was demonstrated that collisions of highly deformed uranium nuclei exhibit elliptic flow patterns which are incompatible with a scale-violating binary collision term postulated by the two-component Glauber ansatz. Measurements of central copper-copper, gold-gold and uranium-uranium particle production at RHIC also exhibit approximate participant scaling in agreement with Eq.~\eqref{eq:homogenous}. Moreover, the scale invariant constraint serves as a reasonable approximation for a number of calculations of the mapping $f$ in Eq.~\eqref{eq:mapping} derived from CGC effective field theory as we show momentarily. At present we thus assert the scale invariant constraint as a simplifying postulate although future work may relax this constraint to further reduce bias in the chosen parameterization.

\fig{trento_events}{
  Several \protect\trento\ Pb+Pb initial condition events for the transverse entropy density $dS/(d^2r_\perp dy)$ calculated using generalized mean parameter $p=0$, nucleon width $w=0.5$~fm, and gamma fluctuation factor $k=1.4$.
}


\fig*{cgc_compare}{
  Profiles of the initial thermal distribution predicted by the KLN (left), EKRT (middle), and wounded nucleon (right) models (dashed black lines) compared to a generalized mean with different values of the parameter $p$ (solid blue lines).
  Staggered lines show different cross sections of the initial entropy density $dS/(d^2r_\perp dy)$ as a function of the participant nucleon density $\T_A$ for several values of $\T_B = 1, 2, 3$ [fm$^{-2}$].
  Entropy normalization is arbitrary.
}

\fig[b]{nch_per_npart}{
  Average charged particle density per participant pair $(d\nch/d\eta)/(\npart/2)$ as a function of participant number for Pb+Pb and p+Pb collisions at $\sqrts=5.02$ TeV.
  Symbols are data from ALICE and lines are model calculations from \protect\trento\ using parameters selected by the Bayesian analysis.
}

\subsection{Reproducing existing I.C.\ models}

The aforementioned procedure defines the \trento\ initial condition model proposed in Ref.~\ref{?}. The model is purposefully constructed to achieve maximal flexibility using a minimal number of parameters and can mimic a wide range initial condition models proposed in the literature. To demonstrate the efficacy of the generalized mean ansatz, we now show that the mapping can reproduce different theory calculations using suitably chosen values of the generalized mean parameter $p$.

Perhaps the simplest and oldest model of heavy-ion initial conditions is the so called participant or wounded nucleon model which deposits a blob of entropy for each nucleon in the nucleus that engages in one or more inelastic collisions. In its Monte Carlo formulation, the wounded nucleon model is described by
\begin{equation}
  s \propto \T_A + \T_B
  \label{eq:wn}
\end{equation}
where $\T_A$ and $\T_B$ denote the event-by-event participant densities defined in Eq.~\eqref{eq:participant}. Quick examination of Eqs.~\eqref{eq:means} and \eqref{eq:wn} shows that the wounded nucleon model describes a subcase of the generalized mean ansatz for $p=1$ up to an arbitrary factor which can be absorbed by the normalization constant.

More sophisticated calculations of the mapping $f$ in Eq.~\eqref{eq:mapping} can be derived from Color-Glass Condensate effective field theory. A common implementation of a CGC based saturation picture is provided by the KLN model where entropy deposition at the QGP thermalization time can be determined from the produced gluon density $s \propto N_g$ where,
\begin{equation}
  \frac{dN_g}{dy\,d^2r_\perp} \sim \Qs{min}^2 \biggl[
    2 + \log \biggl(\frac{\Qs{max}^2}{\Qs{min}^2} \biggr)
  \biggr]
  \label{eq:kln}
\end{equation}
and $\Qs{max}$ and $\Qs{min}$ denote the larger and smaller values of the two saturation scales in opposite nuclei at any fixed position in the transverse plane.
In the original formulation of the KLN model, the two saturation scales are proportional to the local participant nucleon density in each nucleus $Q^2_{s,A} \propto \T_A$, and Eq.~\eqref{eq:kln} can be recast as,
\begin{equation}
  s \sim \min(\T_A, \T_B) \biggl[
    2 + \log \biggl(\frac{\max(\T_A,\T_B)}{\min(\T_A,\T_B)}\biggr)
  \biggr]
\end{equation}
to put it in a form which can be directly compared with the wounded nucleon model.

Another CGC model which has attracted recent interest after it successfully described an extensive list of experimental particle multiplicity and flow observables is the EKRT model which combines collinearly factorized pQCD minijet production with a simple conjecture for gluon saturation. The energy density predicted by the model after a pre-thermal Bjorken free streaming stage is given by
\begin{equation}
  e(\tau_0, x, y) \sim \frac{K_\text{sat}}{\pi} p_\text{sat}^3(T_A, T_B).
  \label{eq:ekrt_energy}
\end{equation}
where the saturation momentum $p_\text{sat}$, defined in Ref.~\ref{?}, is a function of phenomenological model parameters $K_\text{sat}$ and $\beta$. The energy density in Eq.~\eqref{eq:ekrt_energy} can then be recast as an entropy density using the thermodynamic relation ${s \sim e^{3/4}}$ to compare it with the previous models.

Note that Eq.~\eqref{eq:ekrt_energy} is expressed as a function of nuclear thickness $T$ which includes contributions from \emph{all} nucleons inside the nucleus and is different than the participant density $\T$ used previously. In order to express initial condition mappings as functions of a common variable one could, e.g.\ relate $\T$ and $T$ using an analytic wounded nucleon model. The effect of this substitution on the EKRT model, however, is small as the mapping deposits zero entropy if nucleons are non-overlapping effectively removing them from the participant thickness function. We thus replace $T$ with $\T$ in the EKRT model and note that similar results are obtained by recasting the wounded nucleon, KLN and \trento\ models as functions of $T$ using standard Glauber relations.

In Fig.~\ref{fig:cgc_compare} we plot one-dimensional slices of the entropy deposition mapping predicted by the KLN, EKRT and wounded nucleon (WN) models for characteristic values of the participant nucleon density sampled in lead-lead collisions at $\sqrts=2.76$~TeV. The vertically staggered lines in each panel show the change in entropy density deposited as a function of $\T_A$ for several constant values of $\T_B$, while the superimposed dashed lines show the generalized mean ansatz after it has been tuned to fit each model. The figure illustrates the flexibility of the ansatz to reproduce different initial condition calculations and quantify differences between the models in terms of the generalized mean parameter $p$. The KLN model, for example, is described by $p\sim-0.67$ while the EKRT model corresponds to $p \sim 0$ and the WN model $p=1$. Smaller, more negative values of $p$ pull the generalized mean toward a minimum function and hence correspond to models with more extreme gluon saturation effects.

The three models shown in Fig.~\ref{fig:cgc_compare} are by no means an exhaustive list of initial condition models proposed in the literature. Notably absent, for instance, is the highly successful IP-Glasma model which combines IP-Sat CGC initial conditions with classical Yang-Mills dynamics to describe the full pre-equilibrium evolution of produced glasma fields. The dynamical nature of IP-Glasma is difficult to characterize as a mapping ${dS/dy \sim f(\T_A,\T_B)}$, but nevertheless, can be classified according its predicted eccentricity harmonics $\varepsilon_n$ which serve as a unique signature of the model.

Fig.~\ref{fig:ipglasma} compares the eccentricity harmonics predicted by IP-Glasma with \trento\ using a geometric mean ${p=0}$ which was previously shown to reproduce the eccentricity hierarchy of $\varepsilon_2$ and $\varepsilon_3$ in IP-Glasma. Each \trento\ initial condition event is free streamed for $\tau=0.4$~fm/c \ref{?} to mimic the weakly coupled pre-equilibrium dynamics of IP-Glasma and match the evolution time of both models. Both calculations are performed using a nucleon width $w=0.43$~fm and identical Woods-Saxon parameters. The result shows a striking similarity in the eccentricity harmonics predicted by both models out to large values of the impact parameter $b$ where the sub-nucleonic structure of the IP-Glasma model starts to have significant effect. The simularity suggests that \trento\ can effectively reproduce the scaling behavior of the IP-Glasma model, although a more detailed comparison of the two models would be necessary to establish the strength of correspondence illustrated in Fig.~\ref{fig:cgc_compare}.

\fig{ipglasma}{
  Eccentricity harmonics $\varepsilon_2$ and $\varepsilon_3$ as a function of impact parameter $b$ for Pb+Pb collisions at ${\sqrts=2.76}$~TeV.
  IP-Glasma events are evaluated after $\tau=0.4$~fm/c CYM evolution and \protect\trento\ events after $\tau=0.4$~fm/c free streaming.
  \protect\trento\ specific parameters are $p=0$ and $k=1.4$.
  Both models use nucleon-nucleon cross section $\sigma_\text{NN}=6.4$~fm$^2$, nucleon width $w=0.43$~fm, and correlated lead nuclei from Ref.~\ref{?}.
  The eccentricity calculation is performed using the energy density of the IP-Glasma model and $T^{00}$ component of the \protect\trento\ stress energy tensor after free streaming.
}


\section{Parameter estimation}

With the full evolution model in hand, a number of important model parameters---related to both initial-state entropy deposition and the QGP medium---remain undetermined.
These parameters typically correlate among each other and affect multiple observables, hence, if we wish to describe a wide variety of experimental observables, the only option is a simultaneous fit of all parameters.
But, it is not feasible to do this directly, since simulating observables at even a single set of parameter values requires thousands of individual events and significant computation time.

To overcome this limitation, we employ a Bayesian method for parameter estimation with computationally expensive models \cite{OHagan:2006ba,Higdon:2008cmc,Higdon:2014tva,Wesolowski:2015fqa}.
Briefly, the model is evaluated at a relatively small \order 2 number of parameter points, the output is interpolated by a Gaussian process emulator, and the emulator is used to systematically explore parameter space with Markov chain Monte Carlo.
This section summarizes the methodology; see Ref.~\cite{Bernhard:2015hxa} for a complete treatment.

\subsection{Model parameters and observables}

We choose a set of nine model parameters for estimation.
Four control the parametric initial state:
\begin{enumerate}
  \item the overall normalization factor,
  \item entropy deposition parameter $p$ from the generalized mean ansatz Eq.~\eqref{eq:genmean},
  \item multiplicity fluctuation gamma shape parameter $k$, and
  \item Gaussian nucleon width $w$, which determines initial-state granularity;
\end{enumerate}
the remaining five are related to the QGP medium:
\begin{enumerate}
  \item[5--7.] the three parameters ($\eta/s$ hrg, min, and slope) in Eq.~\eqref{eq:etas} that set the temperature dependence of shear viscosity,
  \setcounter{enumi}{7}
  \item normalization prefactor for the temperature dependence of bulk viscosity Eq.~\eqref{eq:zetas}, and
  \item hydro-to-UrQMD switching temperature $T_\text{switch}$.
\end{enumerate}
This parameter set will enable simultaneous characterization of the initial state and medium, including any correlations.
Table~\ref{tab:design} summarizes the parameters and their corresponding ranges, which are intentionally wide to ensure that the optimal values are bracketed.

\begin{table}
  \caption{
    \label{tab:design}
    Input parameter ranges for the initial condition and hydrodynamic models.
  }
  \begin{ruledtabular}
  \begin{tabular}{lll}
    Parameter         & Description                        & Range           \\
    \paddedhline
    Norm              & Overall normalization              & 100--250        \\
    $p$               & Entropy deposition parameter       & $-1$ to $+1$    \\
    $k$               & Multiplicity fluct.\ shape         & 0.8--2.2        \\
    $w$               & Gaussian nucleon width             & 0.4--1.0 fm     \\
    $\eta/s$ hrg      & Const.\ shear viscosity, $T < T_c$ & 0.3--1.0        \\
    $\eta/s$ min      & Shear viscosity at $T_c$           & 0--0.3          \\
    $\eta/s$ slope    & Slope above $T_c$                  & 0--2 GeV$^{-1}$ \\
    $\zeta/s$ norm    & Prefactor for $(\zeta/s)(T)$       & 0--2            \\
    $T_\text{switch}$ & Hydro-UrQMD switching temp.        & 135--165 MeV    \\
  \end{tabular}
  \end{ruledtabular}
\end{table}

\begin{table*}
  \caption{
    \label{tab:observables}
    Experimental data to be compared with model calculations.
  }
  \begin{ruledtabular}
  \begin{tabular}{lcccc}
    Observable & Particle species & Kinematic cuts & Centrality classes & Ref. \\
    \paddedhline
    Yields $dN/dy$                       & $\pi^\pm$, $K^\pm$, $p\bar p$ &
    $|y| < 0.5$ & 0--5, 5--10, 10--20, \ldots, 60--70 & \cite{Abelev:2013vea} \\
    \noalign{\smallskip}
    Mean transverse momentum $\avg{p_T}$ & $\pi^\pm$, $K^\pm$, $p\bar p$ &
    $|y| < 0.5$ & 0--5, 5--10, 10--20, \ldots, 60--70 & \cite{Abelev:2013vea} \\
    \noalign{\smallskip}
    Two-particle flow cumulants $\vnk n 2$ & \multirow{2}{*}{all charged} &
    $|\eta| < 1$ & 0--5, 5--10, 10--20, \ldots, 40--50 &
    \multirow{2}{*}{\cite{ALICE:2011ab}} \\
    $n = 2$, 3, 4 & & $0.2 < p_T < 5.0$ GeV & $n = 2$ only: 50--60, 60--70 & \\
  \end{tabular}
  \end{ruledtabular}
\end{table*}

Having designated the model parameters and ranges, we generated a 300 point maximin Latin hypercube design \cite{Morris:1995lh} in the nine-dimensional space and executed \order 4 minimum-bias Pb+Pb events at each of the 300 points.
Each event consists of a single initial condition and hydro simulation followed by multiple samples of the freeze-out hypersurface.
The number of samples is roughly inversely proportional to the event's particle multiplicity so that total particle production is constant across all events---typically ${\sim}$5 samples for central events and up to 100 for peripheral events.
This strategy leads to consistent statistical uncertainties across all parameter points and centrality classes.

Parameter estimation relies on observables that are sensitive to varying the model inputs.
For example, bulk viscosity suppresses radial expansion, so a meaningful estimate of the $(\zeta/s)(T)$ normalization parameter requires some measure of radial flow such as the mean transverse momentum.
Indeed, previous work has shown that finite bulk viscosity is necessary to simultaneously fit both mean transverse momentum and anisotropic flow \cite{Ryu:2015vwa}.

For the present study we compare to the centrality dependence of identified particle yields $dN/dy$ and mean transverse momenta $\avg{p_T}$ for charged pions, kaons, and protons as well as two-particle anisotropic flow coefficients $\vnk n 2$ for $n = 2$, 3, 4.
Table~\ref{tab:observables} summarizes the observables including kinematic cuts, centrality classes, and experimental data, which are all from the ALICE experiment, Pb+Pb collisions at 2.76 TeV \cite{Abelev:2013vea,ALICE:2011ab}.

When computing simulated observables, we strive to replicate experimental methods as closely as possible.
We selected the same centrality classes as the corresponding experimental data by sorting each design point's minimum-bias events by charged-particle multiplicity $d\nch/d\eta$ at mid-rapidity ($|\eta| < 0.5$) and dividing the events into the desired percentile bins.
We computed identified $dN/dy$ and $\avg{p_T}$ by simple counting and averaging of the desired species at mid-rapidity ($|y| < 0.5$); no additional steps are necessary since the experimental data are corrected and extrapolated to zero $p_T$ \cite{Abelev:2013vea}.
Finally, we calculated flow coefficients for charged particles within the kinematic range of the ALICE detector using the direct $Q$-cumulant method \cite{Bilandzic:2010jr}.

The top row of Fig.~\ref{fig:observables_samples} shows the final observables for each of the 300 design points;
their large spreads arise from the wide input parameter ranges.

\subsection{Gaussian process emulators}

\newcommand{\x}{\mathbf x}
\newcommand{\y}{\mathbf y}
\newcommand{\N}{\mathcal N}
\newcommand{\muvec}{\boldsymbol\mu}
\newcommand{\tran}{^\intercal}

Central to the parameter estimation method is a statistical surrogate model that interpolates the model input parameter space and provides fast predictions of the output observables at arbitrary inputs.
We use Gaussian process emulators \cite{Rasmussen:2006gp} as flexible, non-parametric interpolators.
Essentially, this amounts to assuming that the model follows a multivariate normal distribution with mean and covariance functions determined by conditioning on actual model calculations.

The full evolution model takes vectors $\x$ of $n = 9$ inputs and produces a number of outputs (each centrality bin of each observable is an output).
For the moment consider only a single output, e.g.\ pion $dN/dy$ in \mbox{20--30\%} centrality (the specific observable does not matter), and call it $y$.
We have already evaluated the model at $m = 300$ design points, i.e.\ an $m \times n$ design matrix $X = \{\x_1, \ldots, \x_m\}$, and obtained the corresponding $m$ outputs $\y = \{y_1, \ldots, y_m\}$.
Now, we assume that the model is a Gaussian process with some covariance function $\sigma$ and condition it on the training data $(X, \y)$, yielding predictions for the outputs $\y_*$ at some other points $X_*$ within the design range.
The predictive distribution for $\y_*$ is the multivariate normal distribution
\begin{equation}
  \begin{aligned}
    \y_* &\sim \N(\muvec, \Sigma), \\
    \muvec &= \sigma(X_*, X)\sigma(X, X)^{-1}\y, \\
    \Sigma &= \sigma(X_*,X_*) - \sigma(X_*,X)\sigma(X,X)^{-1}\sigma(X,X_*),
  \end{aligned}
\end{equation}
where $\muvec$ is the mean vector and $\Sigma$ the covariance matrix, and the notation $\sigma(\cdot, \cdot)$ indicates a matrix from applying the covariance function to each pair of inputs, e.g.
\begin{equation}
  \sigma(X, X) =
  \begin{pmatrix}
    \sigma(\x_1, \x_1) & \cdots & \sigma(\x_1, \x_m) \\
    \vdots & \ddots & \vdots \\
    \sigma(\x_m, \x_1) & \cdots & \sigma(\x_m, \x_m) \\
  \end{pmatrix}.
\end{equation}
Thus, we obtain both the mean predicted output and corresponding uncertainty at any desired input point.
Generally, the uncertainty is small near explicitly calculated points and wide in gaps, reflecting the true state of knowledge of the interpolation.

The covariance function $\sigma$ quantifies the correlation between pairs of input points.
We use a typical Gaussian function
\begin{equation}
  \sigma(\x, \x') = \sigma_\text{GP}^2 \exp\Biggl[ -\sum_{k=1}^n \frac{(x_k - x'_k)^2}{2\ell_k^2} \Biggr] + \sigma_n^2\delta_{\x\x'},
\end{equation}
which yields smoothly-varying processes with continuous derivatives, making it a common choice for well-behaved models.
This form has several \emph{hyperparameters} to be estimated from the training data:
the overall variance of the Gaussian process $\sigma_\text{GP}^2$,
the correlation lengths for each input parameter $\ell_k$,
and the noise variance $\sigma_n^2$ which allows for statistical error.
These hyperparameters may be estimated from the training data by numerically maximizing the likelihood function
\begin{equation}
  \log P = -\frac{1}{2} \y\tran \Sigma^{-1} \y - \frac{1}{2} \log |\Sigma| - \frac{m}{2} \log 2\pi,
\end{equation}
with $\Sigma = \sigma(X, X)$, i.e.\ the covariance function applied to the inputs.
This form consists of a least-squares fit to the data (first term), a complexity penalty to prevent overfitting (second term), and a normalization constant (third term).

To this point we have considered only a single output.
Gaussian processes are fundamentally scalar functions, but the model produces many outputs, all of which must be emulated.
This is readily solved by transforming the output data into orthogonal and uncorrelated linear combinations called principal components, then emulating each component with an individual Gaussian process.

Let $p$ be the number of model outputs, that is, given an $m \times n$ design matrix $X$, the model produces an $m \times p$ output matrix $Y$.
The principal components $Z$ are then computed by the linear transformation
\begin{equation}
  Z = \sqrt m \, Y U
\end{equation}
where $U$ are the eigenvectors of the sample covariance matrix $Y\tran Y$.
The Gaussian processes predict principal components $Z_*$ at input points $X_*$ which are then transformed back to physical space as
\begin{equation}
  Y_* = \frac{1}{\sqrt m} Z_* U\tran.
\end{equation}

Often, the $p$ model outputs are strongly correlated and so a much smaller number of principal components $q~\ll~p$ account for most of the model's variance.
Thus one can use only $q$ components, reducing a high-dimensional output space to a few one-dimensional problems with negligible loss of information.
We use $q = 8$ principal components, retaining over 99.5\% of the variance from the original $p = 68$ outputs.

To validate the performance of the emulators, we generated an independent 50 point Latin hypercube design from the original design space, evaluated the full model at each validation point, and compared the explicit model calculations to emulator predictions.
Figure~\ref{fig:validation} confirms that the emulators faithfully predict true model calculations.
The predictions need not agree perfectly at every point; ideally the residuals would be normally distributed with mean zero and variance predicted by the Gaussian processes.

\fig*{validation}{
  Validation of Gaussian process emulator predictions.
  Each panel shows predictions compared to explicit model calculations at the 50 validation design points.
  The horizontal location and error bar of each point indicates the predicted value and uncertainty,
  vertical indicates the explicitly calculated value and statistical uncertainty,
  and the diagonal gray line represents perfect agreement.
  Left: charged pion yields $dN_{\pi^\pm}/dy$,
  middle: mean pion transverse momenta $\avg{p_T}_{\pi^\pm}$,
  right: flow cumulant $\vnk 2 2$;
  each in centrality bins 0--5\% (blue) and 30--40\% (orange).
}

\subsection{Bayesian calibration}

\newcommand{\z}{\mathbf z}
\newcommand{\st}{_\star}
\newcommand{\ex}{_\text{exp}}

The final step in the parameter estimation method is to calibrate the model parameters to optimally reproduce experimental observables, thereby extracting probability distributions for the true values of the parameters.
According to Bayes' theorem, the probability for the true parameters $\x\st$is
\begin{equation}
  P(\x\st|X,Y,\y\ex) \propto P(X,Y,\y\ex|\x\st) P(\x\st).
  \label{eq:bayes}
\end{equation}
The left-hand side is the \emph{posterior}: the probability of $\x\st$ given the design $X$, computed observables $Y$, and experimental data $\y\ex$.
On the right-hand side, $P(\x\st)$ is the \emph{prior} probability---encapsulating initial knowledge of $\x\st$---and $P(X,Y,\y\ex|\x\st)$ is the likelihood: the probability of observing $(X, Y, \y\ex)$ given a proposal $\x\st$.

The likelihood may be quickly computed using the principal component Gaussian process emulators constructed in the previous subsection:
\begin{align}
  P &= P(X,Y,\y\ex|\x\st) \nonumber \\
    &= P(X,Z,\z\ex|\x\st) \nonumber \\
    &\propto\exp\biggl\{
      -\frac{1}{2} (\z\st - \z\ex)\tran \Sigma_z^{-1} (\z\st - \z\ex)
    \biggr\},
  \label{eq:likelihood}
\end{align}
where $\z\st = \z\st(\x\st)$ are the principal components predicted by the emulators, $\z\ex$ is the principal component transform of the experimental data $\y\ex$, and $\Sigma_z$ is the covariance (uncertainty) matrix.
As in previous work \cite{Novak:2013bqa,Bernhard:2015hxa}, we assume a simple fractional uncertainty on the principal components, so that the covariance matrix is
\begin{equation}
  \Sigma_z = \text{diag}(\sigma^2_z\,\z\ex),
\end{equation}
with $\sigma_z = 0.10$ in the present study.

We place a uniform prior on the model parameters, i.e.\ the prior is constant within the design range and zero outside.
Combined with the likelihood \eqref{eq:likelihood} and Bayes' theorem \eqref{eq:bayes}, we can easily evaluate the posterior probability at any point in parameter space.

The canonical method for constructing the posterior distribution is Markov chain Monte Carlo (MCMC).
MCMC algorithms generate random walks through parameter space by accepting or rejecting proposal points based on the posterior probability; after many steps the chain converges to the desired posterior.

We use the affine-invariant ensemble sampler \cite{Goodman:2010en,FM:2013mc}, an efficient MCMC algorithm that uses a large ensemble of interdependent walkers.
We first run \order 6 steps to allow the chain to equilibrate, discard these ``burn-in'' samples, then generate \order 7 posterior samples.

\fig*{observables_samples}{
  Identified yields (left column), mean $p_T$ (middle column), and flow cumulants $v_n\{2\}$ (right column).
  The top row shows results obtained from the training data used to condition the emulator.
  from 100 random samples drawn from the Bayesian posterior.
}

\fig*{posterior}{
  Diagonal and lower-diagonal: posterior marginal and joint distributions of the calibration parameters described in section \ref{?}.
  Diagonal entries show the posterior likelihood distribution of each parameter marginalized over remaining parameters, while lower-diagonal entries show the joint distribution of parameter pairs.
  $^\dagger$The units for $\eta/s$ slope are [GeV$^{-1}$].
}


\section{Results}

\begin{table}
  \caption{
    \label{tab:posterior}
    Estimated parameter values (medians) and uncertainties (90\% credible intervals) from the posterior distributions calibrated to identified and charged particle yields (middle and right columns, respectively).
  }
  \begin{ruledtabular}
    \begin{tabular}{lll}
      Calibrated to: & \multicolumn{1}{c}{Identified} & \multicolumn{1}{c}{Charged} \\
      \paddedhline
      \input{tables/posterior}
    \end{tabular}
  \end{ruledtabular}
\end{table}

Figure~\ref{fig:mode_observables} includes an additional centrality bin (70--80\%) that was not used in parameter estimation.

\begin{table}
  \caption{
    \label{tab:mode_params}
    High-probability parameters chosen based on the posterior distributions and used to generate Fig.~\ref{fig:mode_observables}.
    Pairs of values separated by slashes are based on identified / charged particle yields, respectively.
    Single values are the same for both cases.
  }
  \begin{ruledtabular}
    \begin{tabular}{ll@{\hspace{2em}}ll}
      \multicolumn{2}{c}{Initial condition} & \multicolumn{2}{c}{QGP medium} \\
      \paddedhline
      norm & 120. / 129.    & $\eta/s$ min      & 0.08  \\
      $p$  & 0.0            & $\eta/s$ slope    & 0.85 / 0.75 GeV$^{-1}$  \\
      $k$  & 1.5  / 1.6     & $\zeta/s$ norm    & 1.25 / 1.10 \\
      $w$  & 0.43 / 0.49 fm & $T_\text{switch}$ & 0.148 GeV \\
    \end{tabular}
  \end{ruledtabular}
\end{table}

\fig*{mode_observables}{
  Top row: model calculations of identified yields $dN/dy$, mean $p_T$, and flow cumulants $v_n\{2\}$ as a function of collision centrality using high-likelihood parameters (listed in table \ref{tab:mode_params}) determined from the posterior in Fig.~\ref{fig:posterior}.
  Symbols with error bars show data from ALICE and lines show results of the model.
  Bottom row: ratio of the model divided by experiment.
  Gray band indicates 10\% discrepancy between the model and experiment.
}


\fig{posterior_p_arrows}{
  Posterior on the generalized mean parameter $p$ which modulates the intensity of saturation effects in the initial conditions compared to the effective $p$-values needed to fit the KLN, EKRT, and wounded nucleon (WN) models.
}

\fig{etas_estimate}{
  Estimated temperature dependence of the shear viscosity $(\eta/s)(T)$ for $T > T_c = 0.154$ GeV.
  The gray shaded region indicates the prior range for the linear $(\eta/s)(T)$ parametrization Eq.~\eqref{eq:etas},
  the blue line is the median from the posterior distribution,
  and the blue band is a 90\% credible interval.
  The horizontal gray line indicates the KSS bound $\eta/s \geq 1/4\pi$ \cite{Policastro:2001yc,Kovtun:2004de}.
}


\section{Summary and conclusions}

We have determined initial condition and transport properties of the QGP medium produced in relativistic heavy-ion collisions using Bayesian methodology. The primary contribution of the present work is the application of the recently developed \trento\ model to parameterize local entropy deposition in relativistic nuclear collisions and extract high likelihood parameters for the QGP initial conditions while asserting minimal theoretical assumptions. We show that the parameterized initial conditions are highly constrained by bulk observables and that the constraints are robust to the presence of correlated model uncertainties such as the temperature dependent shear and bulk viscosities.

The \trento\ initial condition model introduces a free parameter $p$ which interpolates among different types of entropy deposition schemes, ranging from a simple wounded nucleon model to specific calculations in CGC effective field theory. 


The model also includes more typical parameters such as a Gaussian nucleon width $w$ and a Gamma distribution shape parameter $k$ which modulates the variance of proton-proton multiplicity fluctuations. 

The \trento\ initial conditions are described by four model parameters: an entropy normalization factor, an entropy deposition parameter $p$ which attenuates particle production in, a Gaussian nucleon width $w$ and a Gamma distribution shape parameter $k$ which modulates the variance of proton-proton multiplicity fluctuations. We showed that the parameterization is sufficiently flexible to mimic several different model calculations in the literature and hence can be used to interpolate a subspace of reasonable initial condition 

The heavy-ion collision transport dynamics were simulated using a modern event-by-event hydrodynamic model with temperature dependent shear and bulk viscous corrections, continuum extrapolated lattice equation of state and microscopic hadronic afterburner. We parameterized the QGP shear viscosity using a simple functional form \eqref{eq:etas} which assumes a minimal value at the pseudo-critical phase transition temperature and rises linearly with increasing temperature. For the QGP bulk viscosity, we used the temperature dependent parameterization constructed in Ref and varied the overall normalization factor to investigate different magnitudes of the bulk viscous curve normalization.
The Bayesian analysis reveals a number of interesting features
\begin{enumerate}
    \item The QGP initial conditions are well described by entropy deposition proportional to the geometric mean of local participant nucleon density \eqref{eq:means}.

    \item We see very little sensitivity of the model to variations in the strength of proton-proton multiplicity fluctuations.

    \item We find a preferred nucleon width $w\approx0.43$~fm that is in remarkable agreement with values extracted from experiment \ref{blah}.

    \item The data is unable to individually constrain $(\eta/s)_\text{min}$ and $(\eta/s)_\text{slope}$ at a single beam energy, but it does constrain a linear combination of the two.

    \item The simulation prefers a non-zero bulk viscosity which corroborates the findings in Ref.~\ref{}.

    \item We find a tight constraint on the hydro-to-micro switching temperature ${T_\text{sw} \approx 149}$~MeV which vanishes if identified pion, kaon, and proton yields are replaced with species-blind charged particle yield.
\end{enumerate}

We've selected a set of high likelihood model parameters from the Bayesian analysis and used them to analyze the optimal performance of the model. The resulting discrepancy between model and experiment is less than $\pm10\%$ for mean $p_T$, flow cumulants, and charged particle yields while we observe significant $\sim\!10$--$30\%$ tension in the relative abundances of pions, kaons, and protons when the model is tuned to fit identified yields.

Although one ultimately seeks a first principles description for QGP energy deposition and thermalization, the present study demonstrates that such an understanding is not required to determine QGP medium parameters with quantitative uncertainty. 
\medskip

All models use in this study are open source. The \trento\ model and free streaming package are available at \url{github.com/Duke-QCD/trento}, the iEBE-VISHNU hybrid model at \url{u.osu.edu/vishnu} and latest version of UrQMD can be downloaded from \url{urqmd.org}.

\begin{acknowledgments}
The authors thank Ulrich Heinz, Scott Pratt, Harri Niemi and Bj\"orn Schenke for helpful discussions and clarifications in the production of the manuscript.
\end{acknowledgments}

\bibliography{trento2}

\end{document}
